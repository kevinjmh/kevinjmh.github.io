
 <!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  
    <title>Spark | Manhua</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Manhua">
    

    
    <meta name="description" content="Spark简单之美 | RDD：基于内存的集群计算容错抽象 Spark on YarnSpark 官方提供了三种集群部署方案： Standalone, Mesos, YARN，区别就在于资源管理调度平台不同。 想在已有的Hadoop集群上使用Spark，实现Spark on Yarn只需修改配置文件vi .&#x2F;conf&#x2F;spark-env.sh添加以下内容  export HADOOP_HOME&amp;#">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark">
<meta property="og:url" content="http://kevinjmh.github.io/2017/04/13/CheatSheet/computing/spark/spark/index.html">
<meta property="og:site_name" content="Manhua">
<meta property="og:description" content="Spark简单之美 | RDD：基于内存的集群计算容错抽象 Spark on YarnSpark 官方提供了三种集群部署方案： Standalone, Mesos, YARN，区别就在于资源管理调度平台不同。 想在已有的Hadoop集群上使用Spark，实现Spark on Yarn只需修改配置文件vi .&#x2F;conf&#x2F;spark-env.sh添加以下内容  export HADOOP_HOME&amp;#">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2017-04-13T07:13:13.000Z">
<meta property="article:modified_time" content="2024-12-27T07:24:40.825Z">
<meta property="article:author" content="Manhua">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@jiangmanhua">

    
    <link rel="alternative" href="/atom.xml" title="Manhua" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/pacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/pacman.jpg">
    
    
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"></head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="Manhua" title="Manhua"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Manhua">Manhua</a></h1>
				<h2 class="blog-motto">Never Say Die</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">主页 | Home</a></li>
					
						<li><a href="/archives">归档 | Archives</a></li>
					
						<li><a href="/about">简介 | About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:kevinjmh.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/13/CheatSheet/computing/spark/spark/" title="Spark" itemprop="url">Spark</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Manhua" target="_blank" itemprop="author">Manhua</a>
		
  <p class="article-time">
    <time datetime="2017-04-13T07:13:13.000Z" itemprop="datePublished"> Published 2017-04-13</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
			<ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark"><span class="toc-number">1.</span> <span class="toc-text">Spark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-on-Yarn"><span class="toc-number">2.</span> <span class="toc-text">Spark on Yarn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81"><span class="toc-number">3.</span> <span class="toc-text">测试代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pycharm%E8%AE%BF%E9%97%AEspark"><span class="toc-number">4.</span> <span class="toc-text">pycharm访问spark</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%AE%89%E8%A3%85py4j"><span class="toc-number">4.0.1.</span> <span class="toc-text">1. 安装py4j</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">4.0.2.</span> <span class="toc-text">2. 配置环境变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E5%85%B3%E8%81%94Spark%E5%92%8CHadoop"><span class="toc-number">4.0.3.</span> <span class="toc-text">3. 关联Spark和Hadoop</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-windows%E4%B8%8A%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82"><span class="toc-number">4.0.4.</span> <span class="toc-text">4. windows上的问题。</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame%E5%88%97%E5%A4%AA%E5%A4%9A-Too-many-columns"><span class="toc-number"></span> <span class="toc-text">DataFrame列太多 Too many columns</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MergeDataframe"><span class="toc-number"></span> <span class="toc-text">MergeDataframe</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A7%91%E5%AD%A6%E8%AE%A1%E6%95%B0%E6%B3%95%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%95%B0%E5%80%BC"><span class="toc-number"></span> <span class="toc-text">科学计数法字符串转数值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9E%E6%8E%A5oracle"><span class="toc-number"></span> <span class="toc-text">连接oracle</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Security"><span class="toc-number"></span> <span class="toc-text">Security</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#UI-filter"><span class="toc-number"></span> <span class="toc-text">UI filter</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ACL%E6%96%B9%E5%BC%8F"><span class="toc-number"></span> <span class="toc-text">ACL方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#thrift-server-%E5%A2%9E%E5%8A%A0%E8%B4%A6%E5%AF%86%E9%89%B4%E6%9D%83"><span class="toc-number"></span> <span class="toc-text">thrift server 增加账密鉴权</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Thrift-Server"><span class="toc-number"></span> <span class="toc-text">Thrift Server</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%90%E5%88%B6%E8%BF%94%E5%9B%9E%E7%BB%93%E6%9E%9C%E5%A4%A7%E5%B0%8F"><span class="toc-number"></span> <span class="toc-text">限制返回结果大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E5%90%AC%E6%89%A7%E8%A1%8C"><span class="toc-number"></span> <span class="toc-text">监听执行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E8%B7%AF%E5%BE%84%E6%96%87%E4%BB%B6%E8%BF%87%E7%A8%8B"><span class="toc-number"></span> <span class="toc-text">读取路径文件过程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E8%BF%87%E5%A4%9A%E6%97%B6%E5%8F%AF%E4%BB%A5%E5%B8%A6%E6%9D%A1%E4%BB%B6%E6%9F%A5%E5%88%86%E5%8C%BA"><span class="toc-number"></span> <span class="toc-text">分区过多时可以带条件查分区</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BASpark-Job-History"><span class="toc-number"></span> <span class="toc-text">本地搭建Spark Job History</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%8C%87%E5%AE%9Athrift%E5%9C%B0%E5%9D%80"><span class="toc-number"></span> <span class="toc-text">测试环境指定thrift地址</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#beeline"><span class="toc-number"></span> <span class="toc-text">beeline</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8F%90%E5%8F%96CSV%E6%95%B0%E6%8D%AE"><span class="toc-number"></span> <span class="toc-text">提取CSV数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%94%AF%E6%8C%81json%E6%A0%BC%E5%BC%8F"><span class="toc-number"></span> <span class="toc-text">支持json格式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%8C%E8%BD%AC%E5%88%97"><span class="toc-number"></span> <span class="toc-text">行转列</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8Crunjob"><span class="toc-number"></span> <span class="toc-text">并行runjob</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Built-in-Functions"><span class="toc-number"></span> <span class="toc-text">Built-in Functions</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#list-map-%E5%90%88%E5%B9%B6-UDAF"><span class="toc-number"></span> <span class="toc-text">list&#x2F;map 合并 UDAF</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F"><span class="toc-number"></span> <span class="toc-text">查询使用方式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%A8%E5%86%8Cudf%E5%87%BD%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">注册udf函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#run-SQL-on-File"><span class="toc-number"></span> <span class="toc-text">run SQL on File</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B0%83%E8%AF%95%E4%B8%B4%E6%97%B6%E6%95%B0%E6%8D%AE"><span class="toc-number"></span> <span class="toc-text">调试临时数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%B4%E6%97%B6%E8%A1%A8"><span class="toc-number"></span> <span class="toc-text">临时表</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%81%BF%E5%85%8D%E9%9B%86%E7%BE%A4%E5%A4%96%E6%9C%BA%E5%99%A8%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1java-net-UnknownHostException"><span class="toc-number"></span> <span class="toc-text">避免集群外机器提交任务java.net.UnknownHostException</span></a>
		
		</div>
		
		<h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p><a target="_blank" rel="noopener" href="http://shiyanjun.cn/archives/744.html">简单之美 | RDD：基于内存的集群计算容错抽象</a></p>
<h3 id="Spark-on-Yarn"><a href="#Spark-on-Yarn" class="headerlink" title="Spark on Yarn"></a>Spark on Yarn</h3><p>Spark 官方提供了三种集群部署方案： Standalone, Mesos, YARN，区别就在于资源管理调度平台不同。</p>
<p>想在已有的Hadoop集群上使用Spark，实现Spark on Yarn只需修改配置文件<code>vi ./conf/spark-env.sh</code>添加以下内容</p>
<blockquote>
<p>export HADOOP_HOME&#x3D;&#x2F;share&#x2F;apps&#x2F;hadoop</p>
<p>export HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop</p>
</blockquote>
<p>任务提交主要参数（用好可以充分利用资源）</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th align="right">含义</th>
</tr>
</thead>
<tbody><tr>
<td>–master MASTER_URL</td>
<td align="right">可以是spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn,  yarn-cluster,yarn-client, local</td>
</tr>
<tr>
<td>–deploy-mode DEPLOY_MODE</td>
<td align="right">Driver程序运行的地方，client或者cluster</td>
</tr>
<tr>
<td>–class CLASS_NAME</td>
<td align="right">主类名称，含包名</td>
</tr>
<tr>
<td>–name NAME</td>
<td align="right">Application名称</td>
</tr>
<tr>
<td>–jars JARS</td>
<td align="right">Driver依赖的第三方jar包</td>
</tr>
<tr>
<td>–py-files PY_FILES</td>
<td align="right">用逗号隔开的放置在Python应用程序PYTHONPATH上的.zip,  .egg, .py文件列表</td>
</tr>
<tr>
<td>–files FILES</td>
<td align="right">用逗号隔开的要放置在每个executor工作目录的文件列表</td>
</tr>
<tr>
<td>–executor-memory MEM</td>
<td align="right">executor内存大小，默认1G</td>
</tr>
<tr>
<td>–total-executor-cores NUM</td>
<td align="right">executor使用的总核数，仅限于Spark Alone、Spark on Mesos模式</td>
</tr>
<tr>
<td>–executor-cores NUM</td>
<td align="right">每个executor使用的内核数，默认为1，仅限于Spark on Yarn模式</td>
</tr>
<tr>
<td>–queue QUEUE_NAME</td>
<td align="right">提交应用程序给哪个YARN的队列，默认是default队列，仅限于Spark on Yarn模式</td>
</tr>
<tr>
<td>–num-executors NUM</td>
<td align="right">启动的executor数量，默认是2个，仅限于Spark on Yarn模式</td>
</tr>
</tbody></table>
<p>更多内容，见<a target="_blank" rel="noopener" href="http://www.cnblogs.com/manhua/p/5145225.html">博客文章</a></p>
<h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --master yarn --name spark-test --class org.apache.spark.examples.SparkPi lib/spark-examples*.jar 10</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># test_spark.py</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line">logFile = <span class="string">&quot;C:\spark-2.0.2-bin-hadoop2.6\README.md&quot;</span></span><br><span class="line">sc = SparkContext(<span class="string">&quot;local&quot;</span>,<span class="string">&quot;Simple App&quot;</span>)</span><br><span class="line">logData = sc.textFile(logFile).cache()</span><br><span class="line"></span><br><span class="line">numAs = logData.<span class="built_in">filter</span>(<span class="keyword">lambda</span> s: <span class="string">&#x27;a&#x27;</span> <span class="keyword">in</span> s).count()</span><br><span class="line">numBs = logData.<span class="built_in">filter</span>(<span class="keyword">lambda</span> s: <span class="string">&#x27;b&#x27;</span> <span class="keyword">in</span> s).count()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Lines with a: %i, lines with b: %i&quot;</span>%(numAs, numBs))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># testDFrame.py</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">schema_inference_example</span>(<span class="params">spark</span>):</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load a text file and convert each line to a Row.</span></span><br><span class="line">    lines = sc.textFile(<span class="string">&quot;/user/jiangmanhua/Experiment/url_domain_labeled&quot;</span>)</span><br><span class="line">    parts = lines.<span class="built_in">map</span>(<span class="keyword">lambda</span> l: l.split(<span class="string">&quot;\t&quot;</span>))</span><br><span class="line">    label = parts.<span class="built_in">map</span>(<span class="keyword">lambda</span> p: Row(label_name=p[<span class="number">0</span>], <span class="built_in">type</span>=p[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Infer the schema, and register the DataFrame as a table.</span></span><br><span class="line">    schemaPeople = spark.createDataFrame(label)</span><br><span class="line">    schemaPeople.createOrReplaceTempView(<span class="string">&quot;label&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SQL can be run over DataFrames that have been registered as a table.</span></span><br><span class="line">    teenagers = spark.sql(<span class="string">&quot;SELECT type, count(label_name) as cc FROM label group by type&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The results of SQL queries are Dataframe objects.</span></span><br><span class="line">    <span class="comment"># rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.</span></span><br><span class="line">    teenNames = teenagers.rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> p: <span class="string">&quot;Name: &quot;</span> + p.<span class="built_in">type</span> + <span class="string">&quot;cnt: &quot;</span> + <span class="built_in">str</span>(p.cc)).collect()</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> teenNames:</span><br><span class="line">        <span class="built_in">print</span>(name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession \</span><br><span class="line">        .builder \</span><br><span class="line">        .appName(<span class="string">&quot;Python Spark SQL basic example&quot;</span>) \</span><br><span class="line">        .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>) \</span><br><span class="line">        .getOrCreate()</span><br><span class="line">    schema_inference_example(spark)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="pycharm访问spark"><a href="#pycharm访问spark" class="headerlink" title="pycharm访问spark"></a>pycharm访问spark</h3><h5 id="1-安装py4j"><a href="#1-安装py4j" class="headerlink" title="1. 安装py4j"></a>1. 安装py4j</h5><p>主要两个步骤：安装py4j、配置路径</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Windows</span></span><br><span class="line">pip install py4j</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Linux</span></span><br><span class="line">sudo pip2 install py4j</span><br></pre></td></tr></table></figure>

<h5 id="2-配置环境变量"><a href="#2-配置环境变量" class="headerlink" title="2. 配置环境变量"></a>2. 配置环境变量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME = C:\hadoop-2.6.5</span><br><span class="line">SPARK_HOME = C:\spark-2.0.2-bin-hadoop2.6</span><br></pre></td></tr></table></figure>
<p>也可以在pycharm中选择“Run” -&gt;“Edit Configurations” -&gt;“Environment variables” 增加SPARK_HOME目录以及HADOOP_HOME目录</p>
<h5 id="3-关联Spark和Hadoop"><a href="#3-关联Spark和Hadoop" class="headerlink" title="3. 关联Spark和Hadoop"></a>3. 关联Spark和Hadoop</h5><p> 在<code>$PYTHON_HOME\lib\site-packages</code>下新建<code>pyspark.pth</code>文件内容为pyspark的目录 (如<code>D:\spark-2.0.2-bin-hadoop2.6\python</code>)</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Linux</span><br><span class="line">echo /home/manhua/app/spark/python &gt; /home/manhua/.local/lib/python2.7/site-packages/pyspark.pth  </span><br></pre></td></tr></table></figure>
<h5 id="4-windows上的问题。"><a href="#4-windows上的问题。" class="headerlink" title="4. windows上的问题。"></a>4. windows上的问题。</h5><p>下载<a target="_blank" rel="noopener" href="https://github.com/srccodes/hadoop-common-2.2.0-bin/tree/master/bin">winutils.exe</a>放到C:\hadoop-2.6.5\bin</p>
<p>set spark.sql.cli.print.header&#x3D;true;</p>
<h2 id="DataFrame列太多-Too-many-columns"><a href="#DataFrame列太多-Too-many-columns" class="headerlink" title="DataFrame列太多 Too many columns"></a>DataFrame列太多 Too many columns</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">V1402</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">                  phone:<span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                  infoSelected:<span class="type">Boolean</span></span></span></span><br><span class="line"><span class="params"><span class="class">                  <span class="comment">// ...</span></span></span></span><br><span class="line"><span class="params"><span class="class">                </span>)</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">V1402</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(phone:<span class="type">String</span>, flags:<span class="type">Array</span>[<span class="type">Boolean</span>]): <span class="type">V1402</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">V1402</span>(phone,</span><br><span class="line">      flags(<span class="number">0</span>), flags(<span class="number">1</span>), flags(<span class="number">2</span>), flags(<span class="number">3</span>), flags(<span class="number">4</span>), flags(<span class="number">5</span>), flags(<span class="number">6</span>), flags(<span class="number">7</span>), flags(<span class="number">8</span>),</span><br><span class="line">      flags(<span class="number">9</span>), flags(<span class="number">10</span>), flags(<span class="number">11</span>), flags(<span class="number">12</span>), flags(<span class="number">13</span>), flags(<span class="number">14</span>), flags(<span class="number">15</span>), flags(<span class="number">16</span>), flags(<span class="number">17</span>),</span><br><span class="line">      flags(<span class="number">18</span>), flags(<span class="number">19</span>), flags(<span class="number">20</span>), flags(<span class="number">21</span>), flags(<span class="number">22</span>), flags(<span class="number">23</span>), flags(<span class="number">24</span>), flags(<span class="number">25</span>),</span><br><span class="line">      flags(<span class="number">26</span>), flags(<span class="number">27</span>), flags(<span class="number">28</span>), flags(<span class="number">29</span>), flags(<span class="number">30</span>), flags(<span class="number">31</span>), flags(<span class="number">32</span>), flags(<span class="number">33</span>),</span><br><span class="line">      flags(<span class="number">34</span>), flags(<span class="number">35</span>), flags(<span class="number">36</span>), flags(<span class="number">37</span>), flags(<span class="number">38</span>), flags(<span class="number">39</span>), flags(<span class="number">40</span>), flags(<span class="number">41</span>)</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="MergeDataframe"><a href="#MergeDataframe" class="headerlink" title="MergeDataframe"></a>MergeDataframe</h1><p>新老数据合并，右表有取右表，否则取左表</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._  </span><br><span class="line">df.joinWith(d, df(<span class="string">&quot;slot&quot;</span>) === d(<span class="string">&quot;slot&quot;</span>) &amp;&amp;  </span><br><span class="line">      df(<span class="string">&quot;servicetype&quot;</span>) === d(<span class="string">&quot;servicetype&quot;</span>) &amp;&amp;  </span><br><span class="line">      df(<span class="string">&quot;utype&quot;</span>) === d(<span class="string">&quot;utype&quot;</span>) &amp;&amp;  </span><br><span class="line">      df(<span class="string">&quot;userid&quot;</span>) === d(<span class="string">&quot;userid&quot;</span>) &amp;&amp;  </span><br><span class="line">      df(<span class="string">&quot;id&quot;</span>) === d(<span class="string">&quot;id&quot;</span>), <span class="string">&quot;fullouter&quot;</span>)  </span><br><span class="line">  .map &#123;  </span><br><span class="line">    <span class="keyword">case</span> (left, right) =&gt; &#123;  </span><br><span class="line">      <span class="type">Vcollection</span>.apply(<span class="keyword">if</span> (right != <span class="literal">null</span>) right <span class="keyword">else</span> left)  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;.filter(_.op_type != <span class="number">2</span>)  </span><br><span class="line">  .write.mode(<span class="string">&quot;overwrite&quot;</span>)  </span><br><span class="line">  .orc(<span class="string">s&quot;/tmp/drs_collectino&quot;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="科学计数法字符串转数值"><a href="#科学计数法字符串转数值" class="headerlink" title="科学计数法字符串转数值"></a>科学计数法字符串转数值</h2><p>select cast(cast(‘1.3169590272E11’ as float) as decimal(19,0))<br>select cast(cast(personaldiskspace as float) as decimal(19,0))</p>
<h2 id="连接oracle"><a href="#连接oracle" class="headerlink" title="连接oracle"></a>连接oracle</h2><p><a target="_blank" rel="noopener" href="https://mvnrepository.com/artifact/com.oracle/ojdbc7/12.1.0.2">https://mvnrepository.com/artifact/com.oracle/ojdbc7/12.1.0.2</a><br>将jar包放到 SPARK_HOME&#x2F;jars，启动beeline，!scan查看确认驱动加载成功</p>
<p>beeline -u “jdbc:oracle:thin:@[ip]:[port]&#x2F;srvName” -n xxx -p xxx -e “$sql”<br>如果直接-u连接失败，先进去beeline，再执行!connect ‘jdbc:oracle:thin:@[ip]:[port]&#x2F;srvName’交互输入用户名密码</p>
<p>&#x2F;&#x2F;监听sid服务，表示sid为orcl<br>database.url&#x3D;jdbc:oracle:thin:@171.xxx.96.xx:xxxx:orcl<br>&#x2F;&#x2F;监听服务名，表示服务名为orcl<br>database.url&#x3D;jdbc:oracle:thin:@171.xxx.96.xx:xxxx&#x2F;orcl</p>
<p>SELECT table_name, column_name, data_type<br>  FROM all_tab_cols<br> WHERE table_name &#x3D; ‘表名 ;<br>注意：表名一定要大写。</p>
<p>oracle Date数据列入hive</p>
<ol>
<li>转字符 TO_CHAR(EFECTIVETIME,’yyyy-MM-dd HH24:mi:ss’)</li>
<li>直接插入 EFECTIVETIME  seatunnel导入则只有日期部分</li>
<li>转秒 (EFECTIVETIME - to_date(‘1970-01-01 08:00:00’,’yyyy-mm-dd hh24:mi:ss’))<em>86400<br>问题：数值溢出；科学计数法显示，</em>86400000&#x2F;1000 正常<br> WHERE ROWNUM &lt; 5;</li>
<li>转时间戳 TO_TIMESTAMP(TO_CHAR(EFECTIVETIME,’yyyy-MM-dd HH24:mi:ss’),’yyyy-MM-dd HH24:mi:ss’) efectivetime</li>
</ol>
<h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><h2 id="UI-filter"><a href="#UI-filter" class="headerlink" title="UI filter"></a>UI filter</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.codec.binary.Base64;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.UnsupportedEncodingException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.*;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* spark-defaults.conf</span></span><br><span class="line"><span class="comment">spark.ui.filters=spark.HcyFilter    //filter类</span></span><br><span class="line"><span class="comment">spark.spark.HcyFilter.param.username=foo     //spark-webUI界面登陆的用户    spark.filter类.param.参数名</span></span><br><span class="line"><span class="comment">spark.spark.HcyFilter.param.password=foo	 //spark-webUI界面登陆的密码    spark.filter类.param.参数名</span></span><br><span class="line"><span class="comment">spark.acls.enable=true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">spark-sql --master yarn --jars basic-1.0-SNAPSHOT.jar --conf spark.ui.filters=spark.HcyFilter --conf spark.spark.HcyFilter.param.username=foo --conf spark.spark.HcyFilter.param.password=foo --conf spark.acls.enable=true</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HcyFilter</span> <span class="keyword">implements</span> <span class="title class_">Filter</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">String</span> <span class="variable">username</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">String</span> <span class="variable">password</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">String</span> <span class="variable">realm</span> <span class="operator">=</span> <span class="string">&quot;Protected&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">(FilterConfig filterConfig)</span> <span class="keyword">throws</span> ServletException &#123;</span><br><span class="line">        username = filterConfig.getInitParameter(<span class="string">&quot;username&quot;</span>);</span><br><span class="line">        password = filterConfig.getInitParameter(<span class="string">&quot;password&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">unauthorized</span><span class="params">(HttpServletResponse response, String message)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        response.setHeader(<span class="string">&quot;WWW-Authenticate&quot;</span>, <span class="string">&quot;Basic realm=\&quot;&quot;</span> + realm + <span class="string">&quot;\&quot;&quot;</span>);</span><br><span class="line">        response.sendError(<span class="number">401</span>, message);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">unauthorized</span><span class="params">(HttpServletResponse response)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        unauthorized(response, <span class="string">&quot;Unauthorized - ui filter&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doFilter</span><span class="params">(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain)</span> <span class="keyword">throws</span> IOException, ServletException &#123;</span><br><span class="line">        <span class="type">HttpServletRequest</span> <span class="variable">request</span> <span class="operator">=</span> (HttpServletRequest) servletRequest;</span><br><span class="line">        <span class="type">HttpServletResponse</span> <span class="variable">response</span> <span class="operator">=</span> (HttpServletResponse) servletResponse;</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> <span class="variable">authHeader</span> <span class="operator">=</span> request.getHeader(<span class="string">&quot;Authorization&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (authHeader != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="type">StringTokenizer</span> <span class="variable">st</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(authHeader);</span><br><span class="line">            <span class="keyword">if</span> (st.hasMoreTokens()) &#123;</span><br><span class="line">                <span class="type">String</span> <span class="variable">basic</span> <span class="operator">=</span> st.nextToken();</span><br><span class="line">                <span class="keyword">if</span> (basic.equalsIgnoreCase(<span class="string">&quot;Basic&quot;</span>)) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        <span class="type">String</span> <span class="variable">credentials</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(Base64.decodeBase64(st.nextToken()), <span class="string">&quot;UTF-8&quot;</span>);</span><br><span class="line"></span><br><span class="line">                        <span class="type">int</span> <span class="variable">p</span> <span class="operator">=</span> credentials.indexOf(<span class="string">&quot;:&quot;</span>);</span><br><span class="line">                        <span class="keyword">if</span> (p != -<span class="number">1</span>) &#123;</span><br><span class="line">                            <span class="type">String</span> <span class="variable">_username</span> <span class="operator">=</span> credentials.substring(<span class="number">0</span>, p).trim();</span><br><span class="line">                            <span class="type">String</span> <span class="variable">_password</span> <span class="operator">=</span> credentials.substring(p + <span class="number">1</span>).trim();</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">if</span> (!username.equals(_username) || !password.equals(_password)) &#123;</span><br><span class="line">                                unauthorized(response, <span class="string">&quot;Bad credentials&quot;</span>);</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            filterChain.doFilter(servletRequest, servletResponse);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            unauthorized(response, <span class="string">&quot;Invalid authentication token&quot;</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Error</span>(<span class="string">&quot;Couldn&#x27;t retrieve authentication&quot;</span>, e);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            unauthorized(response);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">destroy</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="ACL方式"><a href="#ACL方式" class="headerlink" title="ACL方式"></a>ACL方式</h2><p>使用YARN时，默认的登陆用户是dr.who（可以在Hadoop的core-site.xml中使用hadoop.http.staticuser.user属性来指定登陆用户）</p>
<p>限制访问<br>[hadoop@wl1 ~]$ spark-shell –master yarn –conf spark.acls.enable&#x3D;true</p>
<p>允许查看<br>[hadoop@wl1 ~]$ spark-shell –master yarn –conf spark.acls.enable&#x3D;true –conf spark.ui.view.acls&#x3D;dr.who</p>
<p>允许kill任务<br>[hadoop@wl1 ~]$ spark-shell –master yarn –conf spark.acls.enable&#x3D;true –conf spark.ui.view.acls&#x3D;dr.who –conf spark.modify.acls&#x3D;dr.who</p>
<p>管理员配置<br>[hadoop@wl1 ~]$ spark-shell –master yarn –conf spark.acls.enable&#x3D;true –conf spark.admin.acls&#x3D;dr.who</p>
<p>也可以按用户组配置，需要关注<code>spark.user.groups.mapping</code></p>
<h2 id="thrift-server-增加账密鉴权"><a href="#thrift-server-增加账密鉴权" class="headerlink" title="thrift server 增加账密鉴权"></a>thrift server 增加账密鉴权</h2><ul>
<li>实现org.apache.hive.service.auth.PasswdAuthenticationProvider</li>
<li>修改&#x2F;新增 {spark_home}&#x2F;conf&#x2F;hive-thrift-site.xml<ul>
<li>hive.server2.authentication&#x3D;CUSTOM (不写到配置中避免影响计算任务)</li>
<li>hive.server2.custom.authentication.class&#x3D;me.jmh.spark.MyAuth</li>
<li>hive.server2.custom.authentication.filepath&#x3D;&#x2F;data&#x2F;soft&#x2F;spark&#x2F;conf&#x2F;thriftAuth</li>
</ul>
</li>
<li>启动参数 –hiveconf hive.server2.authentication&#x3D;CUSTOM</li>
<li>编辑密码账号<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.custom.authentication.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;me.jmh.spark.MyAuth&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.custom.authentication.filepath&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/data/soft/spark/conf/thriftAuth&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="Thrift-Server"><a href="#Thrift-Server" class="headerlink" title="Thrift Server"></a>Thrift Server</h1><h2 id="限制返回结果大小"><a href="#限制返回结果大小" class="headerlink" title="限制返回结果大小"></a>限制返回结果大小</h2><ul>
<li>配置增量返回 <code>spark.sql.thriftServer.incrementalCollect</code>，每批返回1000条</li>
<li>客户端配置最大获取结果量，如zeppelin</li>
</ul>
<h2 id="监听执行"><a href="#监听执行" class="headerlink" title="监听执行"></a>监听执行</h2><p><code>spark.sql.queryExecutionListeners</code></p>
<h1 id="读取路径文件过程"><a href="#读取路径文件过程" class="headerlink" title="读取路径文件过程"></a>读取路径文件过程</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">DataFrameReader</span><br><span class="line">- source=format</span><br><span class="line">- load: DataSource.lookupDataSourceV2(source通过with DataSourceRegister注册，返回ParquetDataSourceV2</span><br><span class="line">- get【Table】FromProvider， schema由文件infer</span><br><span class="line">- 生成计划</span><br><span class="line">Dataset.ofRows(</span><br><span class="line">sparkSession,</span><br><span class="line">DataSourceV2Relation.create(table, catalog-none, ident-none, dsOptions))</span><br><span class="line"></span><br><span class="line">【关键是获得table对象-&gt;转为DataSourceV2Relation】</span><br><span class="line">然后是逻辑计划到执行计划</span><br><span class="line"></span><br><span class="line">newScanBuilder</span><br><span class="line"></span><br><span class="line">format -&gt; datasource -&gt; table -&gt; relation(LogicalPlan) -&gt; newScanBuilder</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">[analyzer]FindDataSourceTable#</span><span class="language-bash">readDataSourceTable</span></span><br><span class="line"><span class="meta prompt_">DataSource#</span><span class="language-bash">resolveRelation</span></span><br><span class="line"></span><br><span class="line">FileSourceStrategy -&gt; FileSourceScanExec (从hadoopFsRelation.fileFormat获得reader:buildReaderWithPartitionValues)-&gt; FileScanRDD</span><br><span class="line"></span><br><span class="line">具体执行读操作的函数buildReaderWithPartitionValues</span><br></pre></td></tr></table></figure>


<h1 id="分区过多时可以带条件查分区"><a href="#分区过多时可以带条件查分区" class="headerlink" title="分区过多时可以带条件查分区"></a>分区过多时可以带条件查分区</h1><p>show partitions db.tbl PARTITION (year&#x3D;2022)<br>支持 and&#x2F;or&#x2F;&gt;&#x2F;&lt;&#x2F;&#x3D;</p>
<h1 id="本地搭建Spark-Job-History"><a href="#本地搭建Spark-Job-History" class="headerlink" title="本地搭建Spark Job History"></a>本地搭建Spark Job History</h1><p>配置 spark.history.fs.logDirectory    为本地某目录，日志文件诸如application_1652535872918_10360.lz4放入即可<br>启动服务 sbin&#x2F;start-history-server.sh<br>访问18080</p>
<h1 id="测试环境指定thrift地址"><a href="#测试环境指定thrift地址" class="headerlink" title="测试环境指定thrift地址"></a>测试环境指定thrift地址</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">  .master(<span class="string">&quot;local[1]&quot;</span>)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .config(<span class="string">&quot;spark.hadoop.hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://192.168.56.112:9083&quot;</span>)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>

<h1 id="beeline"><a href="#beeline" class="headerlink" title="beeline"></a>beeline</h1><p>使用命令<code>beeline -u jdbc:hive2://localhost:10199</code>访问thrift server时，身份为anonymous</p>
<h1 id="提取CSV数据"><a href="#提取CSV数据" class="headerlink" title="提取CSV数据"></a>提取CSV数据</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create table</span> tmp.mytable</span><br><span class="line"><span class="keyword">USING</span> CSV <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="comment">/*+COALESCE(1)*/</span> <span class="operator">*</span> <span class="keyword">from</span> dim.mcloud_behavior</span><br><span class="line">;</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-- source table</span><br><span class="line">create temporary view s using csv options (path &#x27;file:/tmp/spark-kyuubi-source-test.csv&#x27;,header &quot;true&quot;);</span><br><span class="line"></span><br><span class="line">insert overwrite directory using csv options (path &#x27;file:/tmp/spark-kyuubi-target&#x27;, delimiter &#x27;|&#x27;,header &quot;false&quot;, emptyValue &#x27;&#x27;) select * from s;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="支持json格式"><a href="#支持json格式" class="headerlink" title="支持json格式"></a>支持json格式</h1><p>从hive的lib中添加jar包，建表使用</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ROW</span> FORMAT SERDE <span class="string">&#x27;org.apache.hive.hcatalog.data.JsonSerDe&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE</span><br></pre></td></tr></table></figure>

<p>zip -u spark-jar.zip lib&#x2F;hive-hcatalog-core-2.3.9.jar</p>
<h1 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="keyword">convert</span> <span class="type">row</span> <span class="keyword">to</span> <span class="keyword">column</span></span><br><span class="line">spark.sql(</span><br><span class="line">  s&quot;&quot;&quot;</span><br><span class="line">     |select msisdn phone,</span><br><span class="line">     |  substr(msisdn,-1) part,</span><br><span class="line">     |  kv[&#x27;0&#x27;] sdk_core,</span><br><span class="line">     |  kv[&#x27;1&#x27;] wap,</span><br><span class="line">     |  kv[&#x27;2&#x27;] web,</span><br><span class="line">     |  kv[&#x27;3&#x27;] android,</span><br><span class="line">     |  kv[&#x27;4&#x27;] ios,</span><br><span class="line">     |  kv[&#x27;5&#x27;] pc,</span><br><span class="line">     |  kv[&#x27;6&#x27;] guanjia,</span><br><span class="line">     |  kv[&#x27;7&#x27;] wechat,</span><br><span class="line">     |  kv[&#x27;8&#x27;] WP8,</span><br><span class="line">     |  kv[&#x27;9&#x27;] other,</span><br><span class="line">     |  kv[&#x27;10&#x27;] TV,</span><br><span class="line">     |  kv[&#x27;11&#x27;] miniprogram,</span><br><span class="line">     |  kv[&#x27;12&#x27;] smart_hardware,</span><br><span class="line">     |  kv[&#x27;13&#x27;] harmony</span><br><span class="line">     |from (</span><br><span class="line">     |  select msisdn,</span><br><span class="line">     |         str_to_map(concat_ws(&#x27;,&#x27;,collect_set(concat_ws(&#x27;:&#x27;,media_id,last_active_date)))) kv</span><br><span class="line">     |  from $&#123;Table.ads_huadan_user_media_last_active_date_df&#125;</span><br><span class="line">     |  where tp=&#x27;$year$month$day&#x27;</span><br><span class="line">     |  group by msisdn</span><br><span class="line">     |)t</span><br><span class="line">     |&quot;&quot;&quot;.stripMargin)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="并行runjob"><a href="#并行runjob" class="headerlink" title="并行runjob"></a>并行runjob</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> df = spark.read.parquet(<span class="string">&quot;s3://data/type=access/interval=1551484800&quot;</span>).repartition(<span class="number">55</span>)</span><br><span class="line">df.cache()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> executorService = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> latch = <span class="keyword">new</span> <span class="type">CountDownLatch</span>(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span>( _ &lt;- <span class="type">Range</span>(<span class="number">0</span>, <span class="number">5</span>) ) &#123;</span><br><span class="line">  executorService.execute(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> id = <span class="type">UUID</span>.randomUUID().toString()</span><br><span class="line">      df.coalesce(<span class="number">11</span>).write.parquet(<span class="string">s&quot;s3://data/test/<span class="subst">$&#123;id&#125;</span>&quot;</span>)</span><br><span class="line">      latch.countDown()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br><span class="line">executorService.shutdown()    <span class="comment">// 停止接收任务</span></span><br><span class="line">latch.await(<span class="number">1</span>, <span class="type">TimeUnit</span>.<span class="type">HOURS</span>) <span class="comment">// 等待任务计数归零结束</span></span><br></pre></td></tr></table></figure>

<h1 id="Built-in-Functions"><a href="#Built-in-Functions" class="headerlink" title="Built-in Functions"></a>Built-in Functions</h1><p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/sql/">Spark SQL, Built-in Functions (apache.org)</a></p>
<h1 id="list-map-合并-UDAF"><a href="#list-map-合并-UDAF" class="headerlink" title="list&#x2F;map 合并 UDAF"></a>list&#x2F;map 合并 UDAF</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.cmic.mcloud.label.bigtable.v1</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.encoders.<span class="type">ExpressionEncoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">SparkSession</span>, functions, <span class="type">Encoder</span>, <span class="type">Encoders</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder</span><br><span class="line">      .master(<span class="string">&quot;local[1]&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> seq = <span class="type">List</span>((<span class="string">&quot;American Person&quot;</span>, <span class="type">List</span>(<span class="string">&quot;Tom&quot;</span>, <span class="string">&quot;Jim&quot;</span>)),</span><br><span class="line">      (<span class="string">&quot;China Person&quot;</span>, <span class="type">List</span>(<span class="string">&quot;LiLei&quot;</span>, <span class="string">&quot;HanMeiMei&quot;</span>)),</span><br><span class="line">      (<span class="string">&quot;China Person&quot;</span>, <span class="type">List</span>(<span class="string">&quot;Red&quot;</span>, <span class="string">&quot;Blue&quot;</span>)))</span><br><span class="line">    spark.sparkContext.parallelize(seq).toDF(<span class="string">&quot;n&quot;</span>,<span class="string">&quot;l&quot;</span>).createOrReplaceTempView(<span class="string">&quot;t&quot;</span>)</span><br><span class="line">    spark.udf.register(<span class="string">&quot;merge&quot;</span>, functions.udaf(<span class="type">MyListMerge</span>))</span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">         |select n, collect_list(l), merge(l)</span></span><br><span class="line"><span class="string">         |from t</span></span><br><span class="line"><span class="string">         |group by n</span></span><br><span class="line"><span class="string">         |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin).show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> seq2 = <span class="type">List</span>((<span class="string">&quot;American Person&quot;</span>, <span class="type">Map</span>(<span class="string">&quot;Tom&quot;</span>-&gt; <span class="string">&quot;Jim&quot;</span>)),</span><br><span class="line">      (<span class="string">&quot;China Person&quot;</span>, <span class="type">Map</span>(<span class="string">&quot;LiLei&quot;</span>-&gt; <span class="string">&quot;HanMeiMei&quot;</span>)),</span><br><span class="line">      (<span class="string">&quot;China Person&quot;</span>, <span class="type">Map</span>(<span class="string">&quot;Red&quot;</span>-&gt; <span class="string">&quot;Blue&quot;</span>)))</span><br><span class="line">    spark.sparkContext.parallelize(seq2).toDF(<span class="string">&quot;n&quot;</span>, <span class="string">&quot;l&quot;</span>).createOrReplaceTempView(<span class="string">&quot;t2&quot;</span>)</span><br><span class="line">    spark.udf.register(<span class="string">&quot;merge2&quot;</span>, functions.udaf(<span class="type">MyMapMerge</span>))</span><br><span class="line">    <span class="comment">// spark.sql(&quot;create or replace temporary function merge2 as &#x27;com.cmic.mcloud.label.bigtable.v1.udf.HiveMapMerge&#x27;&quot;)</span></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">         |select n, merge2(l)</span></span><br><span class="line"><span class="string">         |from t2</span></span><br><span class="line"><span class="string">         |group by n</span></span><br><span class="line"><span class="string">         |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin).show(<span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyListMerge</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">List</span>[<span class="type">String</span>], <span class="title">List</span>[<span class="type">String</span>], <span class="title">List</span>[<span class="type">String</span>]] </span>&#123;</span><br><span class="line">  <span class="comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>.empty[<span class="type">String</span>]</span><br><span class="line">  <span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></span><br><span class="line">  <span class="comment">// and return it instead of constructing a new object</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buffer: <span class="type">List</span>[<span class="type">String</span>], employee: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">List</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    buffer.++(employee)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Merge two intermediate values</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">List</span>[<span class="type">String</span>], b2: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">List</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    b1.++(b2)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Transform the output of the reduction</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">List</span>[<span class="type">String</span>] = reduction</span><br><span class="line">  <span class="comment">// Specifies the Encoder for the intermediate value type</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">List</span>[<span class="type">String</span>]] = <span class="type">Encoders</span>.product</span><br><span class="line">  <span class="comment">// Specifies the Encoder for the final output value type</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">List</span>[<span class="type">String</span>]] = <span class="type">Encoders</span>.product</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyMapMerge</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>], <span class="title">Map</span>[<span class="type">String</span>,<span class="type">String</span>], <span class="title">Map</span>[<span class="type">String</span>,<span class="type">String</span>]] </span>&#123;</span><br><span class="line">  <span class="comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>] = <span class="type">Map</span>.empty[<span class="type">String</span>,<span class="type">String</span>]</span><br><span class="line">  <span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></span><br><span class="line">  <span class="comment">// and return it instead of constructing a new object</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buffer: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>], employee: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>]): <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>] = &#123;</span><br><span class="line">    buffer ++ employee</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Merge two intermediate values</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>], b2: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>]): <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>] = &#123;</span><br><span class="line">    b1 ++ b2</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Transform the output of the reduction</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>]): <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>] = reduction</span><br><span class="line">  <span class="comment">// Specifies the Encoder for the intermediate value type</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>]] = <span class="type">ExpressionEncoder</span>()</span><br><span class="line">  <span class="comment">// Specifies the Encoder for the final output value type</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>]] = <span class="type">ExpressionEncoder</span>()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 要支持sql注册则使用此类</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DataTypes</span>, <span class="type">MapType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">ArrayList</span>, <span class="type">List</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HiveMapMergeUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> inputFields: <span class="type">List</span>[<span class="type">StructField</span>] = <span class="keyword">new</span> <span class="type">ArrayList</span>[<span class="type">StructField</span>]</span><br><span class="line">  inputFields.add(<span class="type">DataTypes</span>.createStructField(<span class="string">&quot;labelmap&quot;</span>, <span class="type">MapType</span>(<span class="type">StringType</span>, <span class="type">StringType</span>), <span class="literal">true</span>))</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> _inputDataType = <span class="type">DataTypes</span>.createStructType(inputFields)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = _inputDataType</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = _inputDataType</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">MapType</span>(<span class="type">StringType</span>, <span class="type">StringType</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = buffer.update(<span class="number">0</span>, <span class="type">Map</span>.empty[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer.update(<span class="number">0</span>, buffer.getMap(<span class="number">0</span>) ++ input.getMap(<span class="number">0</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1.update(<span class="number">0</span>, buffer1.getMap(<span class="number">0</span>) ++ buffer2.getMap(<span class="number">0</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = buffer.getMap(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>使用: 由多个小map合并成一个大map</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span></span>(spark:<span class="type">SparkSession</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  spark.sql(<span class="string">&quot;create or replace temporary function map_merge as &#x27;com.cmic.mcloud.label.bigtable.common.HiveMapMergeUDAF&#x27;&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map_merge</span></span>(spark:<span class="type">SparkSession</span>, tp:<span class="type">String</span>) = &#123;</span><br><span class="line">  init(spark)</span><br><span class="line">  spark.sql(</span><br><span class="line">      <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">         | select msisdn, map_merge(kv) kv</span></span><br><span class="line"><span class="string">         | from bigtable.basetag_map</span></span><br><span class="line"><span class="string">         | where tp=&#x27;$tp&#x27;</span></span><br><span class="line"><span class="string">         | group by msisdn</span></span><br><span class="line"><span class="string">         |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exportAll</span></span>(spark:<span class="type">SparkSession</span>, tp:<span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">val</span> mapTypeDF = map_merge(spark, tp)</span><br><span class="line">  <span class="comment">// will query all rows</span></span><br><span class="line">  <span class="keyword">val</span> keys = mapTypeDF.select(explode(map_keys(col(<span class="string">&quot;kv&quot;</span>)))).distinct().collect().map(_.get(<span class="number">0</span>))</span><br><span class="line">  <span class="keyword">val</span> keyCols = keys.map(f =&gt; col(<span class="string">&quot;kv&quot;</span>).getItem(f).as(f.toString))</span><br><span class="line">  mapTypeDF.select(col(<span class="string">&quot;msisdn&quot;</span>) +: keyCols: _*)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="查询使用方式"><a href="#查询使用方式" class="headerlink" title="查询使用方式"></a>查询使用方式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select msisdn,tags,tags[&quot;storage_purchase_m&quot;],</span><br><span class="line">  map_contains_key(tags,&#x27;function_purchase_m&#x27;),</span><br><span class="line">  map_filter(tags, (k, v) -&gt; k in (&#x27;function_purchase_m&#x27;,&#x27;pay_flag&#x27;))</span><br><span class="line">  from  bigtable.tag_m</span><br><span class="line">  where tp=202312 and taggroup=&#x27;member_order&#x27;</span><br><span class="line">  and (map_contains_key(tags,&#x27;function_purchase_m&#x27;)</span><br><span class="line">  or map_contains_key(tags,&#x27;pay_flag&#x27;))</span><br><span class="line">  and tags[&#x27;storage_purchase_m&#x27;] &gt; 1</span><br><span class="line">  limit 10;</span><br></pre></td></tr></table></figure>

<p> <code>map_contains_key</code>为新增函数，老版本使用<code>tags[&#39;key&#39;] is not null</code> or <code>array_contains(map_keys(map))</code> </p>
<h1 id="注册udf函数"><a href="#注册udf函数" class="headerlink" title="注册udf函数"></a>注册udf函数</h1><p>1）临时函数<br>在一次会话（Session）中使用如下语句创建临时函数：</p>
<p>add jar hdfs:&#x2F;&#x2F;bicluster&#x2F;dolphinscheduler&#x2F;hadoop&#x2F;resources&#x2F;test&#x2F;jmh&#x2F;label-1.0.jar;<br>create or replace temporary function collect_map as ‘com.cmic.mcloud.label.bigtable.v1.udf.HiveMapMerge’; </p>
<p>select * from bigtable.basetag_map where msisdn &#x3D;’19802021823’;<br>select collect_map(kv) from bigtable.basetag_map where msisdn &#x3D;’19802021823’;</p>
<p>（2）永久函数<br>这个特性需要高版本的Hive支持，它的好处是可以将UDF Jar存放至HDFS，函数仅需要创建一次即可以永久使用，如下：<br>CREATE FUNCTION collect_map AS ‘com.cmic.mcloud.label.bigtable.v1.udf.MyMapMerge’ USING JAR ‘hdfs:&#x2F;&#x2F;bicluster&#x2F;dolphinscheduler&#x2F;hadoop&#x2F;resources&#x2F;test&#x2F;jmh&#x2F;label-1.0.jar’; </p>
<h1 id="run-SQL-on-File"><a href="#run-SQL-on-File" class="headerlink" title="run SQL on File"></a>run SQL on File</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select * from FORMAT.`PATH`</span><br><span class="line"></span><br><span class="line">其中含子目录情况下，path不支持 select * from orc.`/system/hdfs/*`</span><br><span class="line">但支持  select * from orc.`/system/hdfs/2*` </span><br></pre></td></tr></table></figure>

<h1 id="调试临时数据"><a href="#调试临时数据" class="headerlink" title="调试临时数据"></a>调试临时数据</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT a, count(*) as cnt </span><br><span class="line">FROM </span><br><span class="line">VALUES (&#x27;A1&#x27;, &#x27;2021-01-01 00:00:00&#x27;), (&#x27;A1&#x27;, &#x27;2021-01-01 00:04:30&#x27;), (&#x27;A1&#x27;, &#x27;2021-01-01 00:06:00&#x27;), (&#x27;A2&#x27;, &#x27;2021-01-01 00:01:00&#x27;) AS tab(a, b)</span><br><span class="line">group by 1</span><br></pre></td></tr></table></figure>

<h1 id="临时表"><a href="#临时表" class="headerlink" title="临时表"></a>临时表</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table tmp.flat_gd_202410(</span><br><span class="line">msisdn string,</span><br><span class="line">opr_type string,</span><br><span class="line">opr_time string)</span><br><span class="line">using csv options(delimiter &#x27;|&#x27;, header false)</span><br></pre></td></tr></table></figure>

<h1 id="避免集群外机器提交任务java-net-UnknownHostException"><a href="#避免集群外机器提交任务java-net-UnknownHostException" class="headerlink" title="避免集群外机器提交任务java.net.UnknownHostException"></a>避免集群外机器提交任务java.net.UnknownHostException</h1><p>增加配置<br>SPARK_LOCAL_HOSTNAME<br>或 spark.driver.host</p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/BigData/">BigData</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/spark/">spark</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://kevinjmh.github.io/2017/04/13/CheatSheet/computing/spark/spark/" data-title="Spark | Manhua" data-tsina="" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2021/04/09/Doc/carbon/pr/" title="Carbondata PR">
  <strong>上一篇：</strong><br/>
  <span>
  Carbondata PR</span>
</a>
</div>


<div class="next">
<a href="/2017/04/13/CheatSheet/virtualization/docker/"  title="Docker">
 <strong>下一篇：</strong><br/> 
 <span>Docker
</span>
</a>
</div>

</nav>

	

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  
<div class="github-card">
<p class="asidetitle">Github Card</p>
<div class="github-card" data-github="kevinjmh" data-width="220" data-height="119" data-theme="medium">
<script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js" ></script>
</div>
  </div>



  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
		  
			<li><a href="/categories/Application/" title="Application">Application<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/BigData/" title="BigData">BigData<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/CheatSheet/" title="CheatSheet">CheatSheet<sup>21</sup></a></li>
		  
		
		  
			<li><a href="/categories/Info/" title="Info">Info<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/慢话/" title="慢话">慢话<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/carbondata/" title="carbondata">carbondata<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/project/" title="project">project<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/rdma/" title="rdma">rdma<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/software/" title="software">software<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/recommendation/" title="recommendation">recommendation<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/hadoop/" title="hadoop">hadoop<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/docker/" title="docker">docker<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/markdown/" title="markdown">markdown<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/carbon/" title="carbon">carbon<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/spark/" title="spark">spark<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="http://www.cnblogs.com/manhua" target="_blank" title="cnBlogs">cnBlogs</a>
            
          </li>
        
    </ul>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Manhua Jiang. <br/>
			This is my blog,believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/443181876" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/kevinjmh" target="_blank" class="icon-github" title="github"></a>
		
		
		
		<a href="https://twitter.com/jiangmanhua" target="_blank" class="icon-twitter" title="twitter"></a>
		
		
		<a href="https://www.facebook.com/jiangmanhua" target="_blank" class="icon-facebook" title="facebook"></a>
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2024 
		
		<a href="/about" target="_blank" title="Manhua">Manhua</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
        getSize();
        if (myWidth >= 1024) {
          c.click();
        }
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>









<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
